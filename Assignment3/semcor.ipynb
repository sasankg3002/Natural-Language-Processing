{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sainath/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sainath/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package semcor to /home/sainath/nltk_data...\n",
      "[nltk_data]   Package semcor is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sainath/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sainath/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this only once\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"semcor\") # downloads the .zip file, but doesn't unzip it\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeyedvectors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[1;32m      2\u001b[0m W2V \u001b[38;5;241m=\u001b[39m KeyedVectors\u001b[38;5;241m.\u001b[39mload_word2vec_format(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoogleNews-vectors-negative300.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m, binary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/__init__.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m4.2.0\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m'\u001b[39m\u001b[39mgensim\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m logger\u001b[39m.\u001b[39mhandlers:  \u001b[39m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/corpora/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mindexedcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m IndexedCorpus  \u001b[39m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmmcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m MmCorpus  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbleicorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m BleiCorpus  \u001b[39m# noqa:F401\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/corpora/indexedcorpus.py:14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m interfaces, utils\n\u001b[1;32m     16\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mIndexedCorpus\u001b[39;00m(interfaces\u001b[39m.\u001b[39mCorpusABC):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/interfaces.py:19\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[39mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m utils, matutils\n\u001b[1;32m     22\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCorpusABC\u001b[39;00m(utils\u001b[39m.\u001b[39mSaveLoad):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/matutils.py:1031\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m1.\u001b[39m \u001b[39m-\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39mlen\u001b[39m(set1 \u001b[39m&\u001b[39m set2)) \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(union_cardinality)\n\u001b[1;32m   1029\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1030\u001b[0m     \u001b[39m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[0;32m-> 1031\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_matutils\u001b[39;00m \u001b[39mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[1;32m   1033\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m   1034\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mlogsumexp\u001b[39m(x):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/_matutils.pyx:1\u001b[0m, in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "W2V = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import semcor # corpus reader: https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/semcor.py\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from string import punctuation\n",
    "from num2words import num2words\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def cosineSimilarity(a, b):\n",
    "    cs = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    return cs\n",
    "\n",
    "def isNumber(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def n2w(w):\n",
    "    # converts given n to word form if n is numeric\n",
    "    if isNumber(w) and w.lower() != \"infinity\" and w.lower() != \"nan\":\n",
    "        w = num2words(w)\n",
    "    return w\n",
    "\n",
    "def lemmatize(w, tag):\n",
    "    if tag is None:\n",
    "        return lemmatizer.lemmatize(w)\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(w, tag)\n",
    "\n",
    "# def clean(tokens):\n",
    "#     tagged = nltk.pos_tag(tokens)\n",
    "#     lemmatized = [lemmatize(w, treebank2wn(tag)) for w, tag in tagged]\n",
    "#     cleaned = [n2w(w) for w in lemmatized if w.lower() not in SW]\n",
    "#     return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DET'), ('is', 'VERB'), ('a', 'DET'), ('doll', 'NOUN')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pos_tag(word_tokenize(\"This is a doll\"),tagset='universal')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_107127/1565140508.py:2: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  W2V.word_vec(\"Hello\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.05102539,  0.12060547, -0.01257324,  0.03088379, -0.08544922,\n",
       "        0.0534668 , -0.13964844, -0.16308594,  0.08300781,  0.20507812,\n",
       "       -0.09423828,  0.15527344, -0.00418091,  0.02111816, -0.10986328,\n",
       "        0.22460938,  0.22265625,  0.15917969,  0.05786133, -0.15527344,\n",
       "        0.23046875,  0.34765625,  0.44335938, -0.14550781,  0.15136719,\n",
       "        0.02209473, -0.16308594,  0.2578125 ,  0.20019531,  0.06054688,\n",
       "       -0.0222168 , -0.13183594, -0.06396484, -0.12792969,  0.09814453,\n",
       "       -0.13574219,  0.01928711,  0.25195312,  0.14257812,  0.2421875 ,\n",
       "        0.03173828, -0.17089844,  0.26171875,  0.4296875 ,  0.24804688,\n",
       "       -0.10351562, -0.13867188, -0.14257812, -0.20703125, -0.00860596,\n",
       "       -0.43359375, -0.01757812,  0.40234375,  0.32617188,  0.33203125,\n",
       "        0.01000977, -0.11523438, -0.12695312,  0.01940918, -0.25390625,\n",
       "        0.00393677, -0.00344849, -0.19042969,  0.05419922,  0.14453125,\n",
       "       -0.0546875 , -0.11132812,  0.15136719, -0.265625  ,  0.20019531,\n",
       "        0.14941406,  0.16601562,  0.06835938,  0.00616455, -0.35742188,\n",
       "        0.14746094,  0.05761719,  0.00152588,  0.12890625,  0.44921875,\n",
       "        0.10302734,  0.06225586, -0.17382812, -0.34570312, -0.10644531,\n",
       "       -0.07324219, -0.05566406,  0.10351562,  0.38867188,  0.2109375 ,\n",
       "       -0.09228516, -0.02709961, -0.03320312, -0.02575684, -0.53125   ,\n",
       "        0.12988281,  0.2578125 ,  0.06494141, -0.04394531, -0.17089844,\n",
       "       -0.31445312, -0.09619141, -0.109375  ,  0.21191406,  0.13183594,\n",
       "        0.14550781,  0.05151367,  0.01275635,  0.07519531,  0.04345703,\n",
       "       -0.00854492,  0.22070312,  0.00055695, -0.23339844,  0.23632812,\n",
       "       -0.05932617,  0.17578125,  0.26953125,  0.06640625, -0.04785156,\n",
       "        0.38671875, -0.00411987, -0.453125  ,  0.06689453, -0.09326172,\n",
       "        0.03637695, -0.11474609,  0.18066406,  0.23632812, -0.11572266,\n",
       "       -0.3203125 , -0.12695312, -0.11474609,  0.10986328,  0.00592041,\n",
       "       -0.234375  , -0.04931641,  0.15429688,  0.09521484,  0.05395508,\n",
       "        0.26171875,  0.14160156, -0.21386719,  0.00747681,  0.12792969,\n",
       "        0.08886719,  0.09570312, -0.3203125 ,  0.22949219, -0.296875  ,\n",
       "        0.02844238,  0.01989746,  0.31054688, -0.17382812,  0.05981445,\n",
       "       -0.1875    , -0.17871094,  0.18164062, -0.12792969, -0.23925781,\n",
       "       -0.375     ,  0.19042969,  0.234375  ,  0.03173828,  0.14160156,\n",
       "       -0.21875   , -0.12695312, -0.03613281, -0.19042969,  0.01660156,\n",
       "        0.33203125, -0.06445312,  0.16210938, -0.16210938, -0.20019531,\n",
       "        0.37890625,  0.04589844, -0.09423828, -0.12597656, -0.09765625,\n",
       "       -0.04248047, -0.1640625 ,  0.0534668 ,  0.26171875,  0.14160156,\n",
       "       -0.2109375 , -0.1484375 ,  0.2578125 , -0.10986328, -0.11279297,\n",
       "       -0.05297852,  0.12695312,  0.3359375 ,  0.12890625,  0.24609375,\n",
       "        0.09472656, -0.0703125 , -0.13183594, -0.05273438, -0.06591797,\n",
       "        0.07324219, -0.02380371, -0.00460815,  0.22363281, -0.03662109,\n",
       "        0.03173828, -0.23632812,  0.296875  , -0.09667969,  0.19042969,\n",
       "        0.09082031,  0.13183594, -0.07226562, -0.28515625,  0.12158203,\n",
       "        0.01287842,  0.21289062, -0.13476562, -0.08496094, -0.37695312,\n",
       "       -0.01483154,  0.30859375, -0.09326172, -0.26367188,  0.03369141,\n",
       "       -0.02880859, -0.09082031, -0.125     , -0.50390625, -0.1640625 ,\n",
       "       -0.12890625, -0.19726562, -0.36523438, -0.04370117, -0.07519531,\n",
       "        0.11035156,  0.03149414,  0.09521484,  0.09033203,  0.11132812,\n",
       "        0.09667969,  0.07275391, -0.04125977,  0.07519531,  0.03613281,\n",
       "        0.05932617,  0.16894531,  0.15039062,  0.06542969, -0.04956055,\n",
       "       -0.05932617,  0.08935547,  0.08154297,  0.18945312,  0.00390625,\n",
       "       -0.08154297, -0.11669922, -0.09326172, -0.32617188, -0.16699219,\n",
       "       -0.0559082 ,  0.13476562,  0.1328125 ,  0.0625    ,  0.07080078,\n",
       "        0.13378906, -0.16210938,  0.15917969,  0.09082031, -0.06542969,\n",
       "       -0.08984375,  0.25390625,  0.19921875, -0.14160156,  0.36914062,\n",
       "        0.14453125, -0.09130859, -0.03564453, -0.01281738, -0.046875  ,\n",
       "       -0.17773438, -0.00149536, -0.13085938,  0.14355469,  0.3671875 ,\n",
       "       -0.17773438, -0.22558594, -0.10986328,  0.04248047,  0.16601562,\n",
       "        0.00964355, -0.06835938, -0.11669922, -0.15136719,  0.36132812,\n",
       "       -0.22851562, -0.01672363, -0.26367188, -0.10742188,  0.0222168 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only for nouns\n",
    "W2V.word_vec(\"Hello\")\n",
    "\n",
    "# print(type(W2V.word_vec(\"Hello\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/sasank/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bank.n.01'),\n",
       " Synset('depository_financial_institution.n.01'),\n",
       " Synset('bank.n.03'),\n",
       " Synset('bank.n.04'),\n",
       " Synset('bank.n.05'),\n",
       " Synset('bank.n.06'),\n",
       " Synset('bank.n.07'),\n",
       " Synset('savings_bank.n.02'),\n",
       " Synset('bank.n.09'),\n",
       " Synset('bank.n.10')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"bank\",'n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STATEMENT FOR INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"Little John was looking for his toy box\"\n",
    "statement = \"The river is flooding on the bank\"\n",
    "\n",
    "pos_tagged_statement =  pos_tag(word_tokenize(statement),tagset='universal')\n",
    "statement = statement.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('river', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('flooding', 'VERB'),\n",
       " ('on', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('bank', 'NOUN')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagged_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_embed(statement,word):\n",
    "    context_embedding = np.zeros(300)\n",
    "    for i in statement:\n",
    "        if i!=word:\n",
    "            try:\n",
    "                context_embedding = context_embedding + W2V.word_vec(i)\n",
    "            except:\n",
    "                continue\n",
    "    statemen_len = len(statement)\n",
    "    return context_embedding/(statemen_len-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sense_embed(synset_id):\n",
    "    sense_embedding = np.zeros(300)\n",
    "    text = synset_id.definition()\n",
    "    re.sub(r\"[^-9A-Za-z ]\", \"\" , text)\n",
    "    text = text.replace(\"(\",\"\")\n",
    "    text = text.replace(\")\",\"\")\n",
    "    text = text.replace(\";\",\"\")\n",
    "    text = text.split()\n",
    "    sense_embedding = context_embed(text,\"\")\n",
    "    # print(text)\n",
    "    return sense_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "river  :  a large natural stream of water (larger than a creek)\n",
      "bank  :  sloping land (especially the slope beside a body of water)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_107127/1121871512.py:6: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  context_embedding = context_embedding + W2V.word_vec(i)\n"
     ]
    }
   ],
   "source": [
    "words_sense_noun = []\n",
    "\n",
    "for i in pos_tagged_statement:\n",
    "    tag = i[1]\n",
    "    word = i[0]\n",
    "    if(tag != \"NOUN\"):\n",
    "        continue\n",
    "    # words_sense_noun.append(word)\n",
    "    cont_embed = context_embed(statement,i)\n",
    "    sen_embed = []\n",
    "    poss_senses = wn.synsets(word,'n')\n",
    "    # print(poss_senses)\n",
    "    for i in poss_senses:\n",
    "        text_hyper = i.hypernyms()\n",
    "        text_hypo = i.hyponyms()\n",
    "        # print(\"Hyper :\",text_hyper)\n",
    "        # print(\"Hypo :\",text_hypo)\n",
    "        # print(i.definition())\n",
    "        x = sense_embed(i)\n",
    "        for j in text_hypo:\n",
    "            x += sense_embed(j)\n",
    "        for j in text_hyper:\n",
    "            x += sense_embed(j)\n",
    "        sen_embed.append(x/(1+len(text_hyper)+len(text_hypo)))\n",
    "    cosine_similarity = []\n",
    "    for i in sen_embed:\n",
    "        cosine_similarity.append(cosineSimilarity(cont_embed,i))\n",
    "    max_similar_sense = poss_senses[np.argmax(cosine_similarity)]\n",
    "    print(word,\" : \",max_similar_sense.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = semcor.tagged_sents(tag = \"sem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'svgling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/formatters.py:343\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    <a href='file:///home/sasank/.local/lib/python3.8/site-packages/IPython/core/formatters.py?line=340'>341</a>\u001b[0m     method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n\u001b[1;32m    <a href='file:///home/sasank/.local/lib/python3.8/site-packages/IPython/core/formatters.py?line=341'>342</a>\u001b[0m     \u001b[39mif\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/sasank/.local/lib/python3.8/site-packages/IPython/core/formatters.py?line=342'>343</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m method()\n\u001b[1;32m    <a href='file:///home/sasank/.local/lib/python3.8/site-packages/IPython/core/formatters.py?line=343'>344</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/sasank/.local/lib/python3.8/site-packages/IPython/core/formatters.py?line=344'>345</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/tree/tree.py:783\u001b[0m, in \u001b[0;36mTree._repr_svg_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/sasank/.local/lib/python3.8/site-packages/nltk/tree/tree.py?line=781'>782</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_repr_svg_\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/sasank/.local/lib/python3.8/site-packages/nltk/tree/tree.py?line=782'>783</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39msvgling\u001b[39;00m \u001b[39mimport\u001b[39;00m draw_tree\n\u001b[1;32m    <a href='file:///home/sasank/.local/lib/python3.8/site-packages/nltk/tree/tree.py?line=784'>785</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m draw_tree(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_repr_svg_()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'svgling'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree(Lemma('state.v.01.say'), ['said'])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][2]#???? What to do here showing error for data[0][1,2,4..]\n",
    "#Please look into this and also how to extract nouns from this and all ...."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
